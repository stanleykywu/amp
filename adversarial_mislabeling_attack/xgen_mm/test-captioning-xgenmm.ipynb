{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ac002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForVision2Seq, AutoTokenizer, StoppingCriteria\n",
    "\n",
    "def get_caption(img, blip_model, blip_image_processor, blip_tokenizer, query=\"\"):\n",
    "    # define the prompt template\n",
    "    def apply_prompt_template(prompt):\n",
    "        s = (\n",
    "            \"<|system|>\\nA chat between a curious user and an artificial intelligence assistant. \"\n",
    "            \"The assistant gives helpful, detailed, and polite answers to the user's questions.<|end|>\\n\"\n",
    "            f\"<|user|>\\n<image>\\n{prompt}<|end|>\\n<|assistant|>\\n\"\n",
    "        )\n",
    "        return s\n",
    "\n",
    "    class EosListStoppingCriteria(StoppingCriteria):\n",
    "        def __init__(self, eos_sequence=[32007]):\n",
    "            self.eos_sequence = eos_sequence\n",
    "\n",
    "        def __call__(\n",
    "            self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n",
    "        ) -> bool:\n",
    "            last_ids = input_ids[:, -len(self.eos_sequence) :].tolist()\n",
    "\n",
    "            return self.eos_sequence in last_ids\n",
    "\n",
    "    inputs = blip_image_processor(\n",
    "        [img], return_tensors=\"pt\", image_aspect_ratio=\"pad\"\n",
    "    )\n",
    "    prompt = apply_prompt_template(query)\n",
    "    language_inputs = blip_tokenizer([prompt], return_tensors=\"pt\")\n",
    "    inputs.update(language_inputs)\n",
    "    inputs = {name: tensor.cuda() for name, tensor in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_text = blip_model.generate(\n",
    "            **inputs,\n",
    "            image_size=[img.size],\n",
    "            pad_token_id=blip_tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=77,\n",
    "            top_p=None,\n",
    "            num_beams=1,\n",
    "            stopping_criteria=[EosListStoppingCriteria()],\n",
    "        )\n",
    "        prediction = blip_tokenizer.decode(\n",
    "            generated_text[0], skip_special_tokens=True\n",
    "        ).split(\"<|end|>\")[0]\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# model setup\n",
    "model_name_or_path = \"Salesforce/xgen-mm-phi3-mini-instruct-r-v1\"\n",
    "blip_model = AutoModelForVision2Seq.from_pretrained(model_name_or_path, trust_remote_code=True).to(\"cuda\")\n",
    "blip_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True, use_fast=False, legacy=False\n",
    ")\n",
    "blip_image_processor = AutoImageProcessor.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True\n",
    ")\n",
    "blip_tokenizer = blip_model.update_special_tokens(blip_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./amp-xgenmm.png\")\n",
    "caption = get_caption(img, blip_model, blip_image_processor, blip_tokenizer, query=\"Describe the image in 20 words or less\")\n",
    "print(caption)\n",
    "display(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amp-blip-3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
